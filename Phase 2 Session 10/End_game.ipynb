{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "End game.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjr4GnCHuXFg",
        "colab_type": "text"
      },
      "source": [
        "# Package installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjAB95PjucZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ed49217c-c035-4fbb-f035-99bdc9ad0771"
      },
      "source": [
        "%pip install pybullet gym[atari,box2d]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.7.3)\n",
            "Requirement already satisfied: gym[atari,box2d] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (1.18.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (1.5.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (7.0.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d]) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,box2d]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpgQqPDOuOTp",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGJBR6vXuOT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pybullet_envs\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image, ImageOps, ImageDraw\n",
        "from collections import deque\n",
        "\n",
        "import gym\n",
        "from gym import error, spaces, utils, wrappers\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwL2RxKzuOUr",
        "colab_type": "text"
      },
      "source": [
        "# Custom gym environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX4qi5q7uOUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CityMap(gym.Env):\n",
        "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
        "    reward_range = (-float('inf'), float('inf'))\n",
        "    spec = None\n",
        "    \n",
        "    observation_window_size = 80\n",
        "    # observation_window_size is the side length of the square surrounding the car\n",
        "    # The car would see (observation_window_size/2) ahead and behind and (observation_window_size/2) to the left and right\n",
        "    \n",
        "    max_turn_radians = np.pi/36.0\n",
        "    # pi/6 radians = 180/36 = 5 degrees\n",
        "    # The car can turn 5 degrees to the left or right    \n",
        "    \n",
        "    distance_threshold_done = 30\n",
        "    # Distance to target to reach before considering the episode done\n",
        "    \n",
        "    goal_circle_radius = int(distance_threshold_done/2)\n",
        "\n",
        "    #Max steps before we call the episode done\n",
        "    max_episode_steps = 10000\n",
        "    \n",
        "    def __init__(self, citymap, roadmask, car_image):\n",
        "        self.action_space = spaces.Box(low = np.float64(-self.max_turn_radians), high = np.float64(self.max_turn_radians), shape = (1,) ) \n",
        "        # Action space is how many degrees to turn the car in radians\n",
        "        \n",
        "        self.observation_space = spaces.Box(low = 0, high = 255, shape = (self.observation_window_size, self.observation_window_size) ) \n",
        "        # All combinations of white and black pixels of observation_window_size x observation_window_size\n",
        "        \n",
        "        self.state = None\n",
        "        \n",
        "        self.citymap = citymap.copy()\n",
        "        self.roadmask = roadmask.copy()\n",
        "        self.car_image = car_image.copy()\n",
        "        \n",
        "        #Find size of the roadmask for reference later\n",
        "        self.roadmask_size_x, self.roadmask_size_y = self.roadmask.getbbox()[2:4]\n",
        "        \n",
        "        # Pad the road mask image to allow for rotations\n",
        "        # Amount of padding required = ( diagonal length of the observation window )/2\n",
        "        self.padding_size = int(self.observation_window_size/np.sqrt(2))\n",
        "        padding = ( self.padding_size, self.padding_size, self.padding_size, self.padding_size )\n",
        "        self.roadmaskpadded = ImageOps.expand( self.roadmask, padding, fill = 255 ) # Pad and fill with sand\n",
        "        \n",
        "        #Set goal point\n",
        "        self.goal_x = 1154\n",
        "        self.goal_y = 158\n",
        "        \n",
        "        self.car_pos_x = 0\n",
        "        self.car_pos_y = 0\n",
        "\n",
        "        self.num_steps = 0\n",
        "        \n",
        "        self.reset()\n",
        "\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "        \n",
        "        Returns:\n",
        "            ( next_state, reward, done, info )\n",
        "        \n",
        "    \"\"\"\n",
        "    def step(self, action):\n",
        "\n",
        "        # Things to compute\n",
        "        # 1. Next position\n",
        "        # 2. Screen grab from next position ( Next state )\n",
        "        # 3. Reward on moving to next position\n",
        "        # 4. Update number of steps taken\n",
        "        # 5. Is the episode done\n",
        "        # 6. Any info to pass on to the agent\n",
        "        \n",
        "\n",
        "\n",
        "        # 1. Next position\n",
        "        # From (pos_x, pos_y) we move forward with 'speed' steps in the direction 'angle+action'\n",
        "        # New angle of car\n",
        "        self.car_angle = self.car_angle + action\n",
        "        if(self.car_angle < 0):\n",
        "            self.car_angle = (2*np.pi) + self.car_angle\n",
        "        elif(self.car_angle > (2*np.pi)):\n",
        "            self.car_angle = self.car_angle - (2*np.pi)\n",
        "        \n",
        "        # Car speed depends on whether we are riding on sand or not\n",
        "        speed = 5 if self.roadmask.getpixel(( self.car_pos_x, self.car_pos_y )) == 0 else 2\n",
        "        \n",
        "        displacement_x = speed * np.sin( self.car_angle )\n",
        "        displacement_y = -1 * speed * np.cos( self.car_angle )\n",
        "        # Displacement y is negative since the top of the frame is y=0\n",
        "        # Hence if the car is pointing upwards ( oriented at 0 degrees ) then the y values would decrease\n",
        "        \n",
        "        self.car_pos_x = self.car_pos_x + displacement_x\n",
        "        self.car_pos_y = self.car_pos_y + displacement_y\n",
        "        \n",
        "        # Clip position to boundaries of the image\n",
        "        self.car_pos_x = np.clip(self.car_pos_x, 0, self.roadmask_size_x-1) \n",
        "        self.car_pos_y = np.clip(self.car_pos_y, 0, self.roadmask_size_y-1)\n",
        "        \n",
        "        # 2. Screen grab from next position ( Next state )\n",
        "        next_state = self._extract_current_frame()\n",
        "        \n",
        "        # 3. Reward on moving to next position\n",
        "        \n",
        "        new_distance_from_goal = np.sqrt( (self.car_pos_x - self.goal_x)**2 + (self.car_pos_y - self.goal_y)**2 )\n",
        "        \n",
        "        pixel_value_at_car_pos = self.roadmask.getpixel((self.car_pos_x, self.car_pos_y))\n",
        "#         assert pixel_value_at_car_pos in [0,1], \"Pixel values are not exactly 0 or 1\")\n",
        "        \n",
        "        if( pixel_value_at_car_pos == 1 ):\n",
        "            #Currently on sand\n",
        "            # reward = -1\n",
        "            reward = -100 * ((new_distance_from_goal+100)/1650) # 1650 is the length of the diagonal of the image\n",
        "        elif( new_distance_from_goal > self.distance_from_goal ):\n",
        "            # reward = -0.2\n",
        "            reward = -10.2 * (new_distance_from_goal/1650)\n",
        "        elif ( new_distance_from_goal == self.distance_from_goal ):\n",
        "            # In one of the corners and driving into the corner\n",
        "            reward = -100\n",
        "        else:\n",
        "            # new_distance_from_goal < self.distance_from_goal\n",
        "            # reward = 0.1\n",
        "            reward = 0.2 * ((1650-new_distance_from_goal)/1650)\n",
        "\n",
        "        \n",
        "        self.distance_from_goal = new_distance_from_goal\n",
        "\n",
        "        # 4. Update number of steps taken\n",
        "\n",
        "        self.num_steps += 1\n",
        "        \n",
        "        # 5. Is the episode done?\n",
        "        \n",
        "        if( new_distance_from_goal < self.distance_threshold_done  or self.num_steps == self.max_episode_steps ):\n",
        "            # Either we have reached the target position or we have exceed the max steps for this episode\n",
        "            done = 1\n",
        "            self.reset()\n",
        "            next_state = np.expand_dims( \n",
        "                np.expand_dims( \n",
        "                    np.zeros( self.observation_window_size**2 ).reshape((self.observation_window_size,self.observation_window_size) ),\n",
        "                    axis = 0 \n",
        "                ),\n",
        "                axis = 0 )\n",
        "        else:\n",
        "            done = 0\n",
        "\n",
        "        # 6. Any info to pass on to the agent\n",
        "\n",
        "        # print(\n",
        "        #     \"x: \"+ str(self.car_pos_x.round(0)) + \n",
        "        #     \"; y: \" + str(self.car_pos_y.round(0)) + \n",
        "        #     \"; angle(deg): \" + str(np.round(self.car_angle*180/np.pi),2) + \n",
        "        #     \"; action: \" + str(np.round(action*180/np.pi,2)) +\n",
        "        #     \"; reward: \" + str(reward)\n",
        "        #     )\n",
        "        \n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    \"\"\"\n",
        "        Extracts the frame that the agent/car currently sees\n",
        "        With respect to the frame extracted the car is always pointing upward\n",
        "        Keeping the orientation fixed is key since else for the same scene( screen grab ), the car can be in different orientations \n",
        "        and hence should take different actions\n",
        "        \n",
        "        For example take the following case \n",
        "            Environment: A single straight road with the car in the middle on the road\n",
        "            Goal: Left end of the road and outside the visibility of the agent\n",
        "        \n",
        "            The car can be oriented left or right and the goal can be to the left or right\n",
        "            <<< NEED TO THINK OF A BETTER EXAMPLE >>>\n",
        "            \n",
        "        Parameters:\n",
        "            None\n",
        "        \n",
        "        Returns:\n",
        "            img - Numpy array of shape ( observation_window_size, observation_window_size )\n",
        "    \"\"\"\n",
        "    def _extract_current_frame(self):\n",
        "        # We know the current position of the car\n",
        "        # Step 1: Extract a square of size observation_window_size*sqrt(2) surrounding the car ( Call this rough cut )\n",
        "        # Step 2: Rotate the rough cut image around the center by angle of the car\n",
        "        # Step 3: Extract a square of size observation_window_size around the center\n",
        "        \n",
        "        \n",
        "        # Step 1: Extract a square of size observation_window_size*sqrt(2) surrounding the car ( Call this rough cut )\n",
        "        # We need to use the padded version of the road mask here\n",
        "        # Hence we add self.padding_size to the x,y position of the car\n",
        "        bounding_box_rough_cut = ( self.car_pos_x, self.car_pos_y, self.car_pos_x+(2*self.padding_size), self.car_pos_y+(2*self.padding_size) )\n",
        "        # print(\"Bounding box of rough cut: \" +str(bounding_box_rough_cut))\n",
        "\n",
        "        rough_cut = self.roadmaskpadded.crop(bounding_box_rough_cut)\n",
        "        \n",
        "        # Step 2: Rotate the rough cut image around the center by angle of the car\n",
        "        \n",
        "        rough_cut_rotated = rough_cut.rotate( self.car_angle * (180/np.pi) )\n",
        "        # PIL's rotate function:\n",
        "        #  - takes input in degrees ( 180 degrees = pi radians; x radians = x*(180/pi) degrees )\n",
        "        #  - by default rotates around the center of the image\n",
        "        #  - rotates anti-clockwise\n",
        "        \n",
        "        # Step 3: Extract a square of size observation_window_size around the center\n",
        "        # Center of the rough cut image is ( rough_cut_size/2, rough_cut_size/2 )\n",
        "        \n",
        "        bounding_box_current_frame = ( \n",
        "            self.padding_size - (self.observation_window_size/2), \n",
        "            self.padding_size - (self.observation_window_size/2), \n",
        "            self.padding_size + (self.observation_window_size/2), \n",
        "            self.padding_size + (self.observation_window_size/2)\n",
        "        )\n",
        "        \n",
        "        current_frame = rough_cut_rotated.crop(bounding_box_current_frame)\n",
        "\n",
        "        return np.expand_dims( np.expand_dims( np.asarray(current_frame)/255, axis = 0 ), axis = 0 )\n",
        "    \n",
        "    def reset(self):\n",
        "        #Randomly initialise the starting position and set velocity\n",
        "        self.car_pos_x = np.random.randint( 0, self.roadmask_size_x )\n",
        "        self.car_pos_y = np.random.randint( 0, self.roadmask_size_y )\n",
        "        # Car position is measured with respect to the road mask ( without padding ). (0,0) is top left\n",
        "        self.car_angle = np.random.default_rng().random() * np.pi * 2.0\n",
        "        # Initial angle ranges from 0 to 2*pi\n",
        "        # Angle measures rotation from vertical axis (i.e angle = 0 when car is heading upwards in the map)\n",
        "        \n",
        "        #Distance from goal\n",
        "        self.distance_from_goal = np.sqrt( (self.car_pos_x - self.goal_x)**2 + (self.car_pos_y - self.goal_y)**2 )\n",
        "\n",
        "        #Set num_steps to 0\n",
        "        self.num_steps = 0\n",
        "        \n",
        "        return self._extract_current_frame()\n",
        "\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        # self.viewer = rendering.SimpleImageViewer()\n",
        "        #Build image of map with goal and car overlaid\n",
        "        \n",
        "        #Create a copy of the map\n",
        "        map_copy = self.citymap.copy()\n",
        "        \n",
        "        #Draw a circle over the goal\n",
        "        draw = ImageDraw.Draw(map_copy)\n",
        "        draw.ellipse( \n",
        "            (self.goal_x - self.goal_circle_radius, \n",
        "             self.goal_y-self.goal_circle_radius, \n",
        "             self.goal_x+self.goal_circle_radius, \n",
        "             self.goal_y+self.goal_circle_radius\n",
        "            ), \n",
        "            fill = 'red', \n",
        "            outline = 'red', \n",
        "            width = 1 \n",
        "        )\n",
        "        del(draw)\n",
        "        \n",
        "        # Create a copy of the car and rotate it to the currrent orientation according to the env state\n",
        "        # Using 90 - curr_angle since the car image oriented horizontally while our angles are from the vertical\n",
        "        car_image_copy = self.car_image.copy().rotate( 360 - (self.car_angle*180/np.pi), expand = True )\n",
        "        car_size_x, car_size_y = car_image_copy.getbbox()[2:4] # The last 2 coordinates represent the size of the car\n",
        "        \n",
        "        #Overlay the car on the map ( copy )\n",
        "        map_copy.paste( car_image_copy, box = ( int(self.car_pos_x - (car_size_x/2)), int(self.car_pos_y - (car_size_y/2)) ) )\n",
        "        del(car_image_copy)\n",
        "        del(car_size_x)\n",
        "        del(car_size_y)        \n",
        "        \n",
        "        current_frame = Image.fromarray( self._extract_current_frame().squeeze(0).squeeze(0)*255 ).convert('RGB')\n",
        "        \n",
        "        if mode == 'rgb_array':\n",
        "            # return np.asarray(current_frame)\n",
        "            return np.asarray(map_copy)\n",
        "#         elif mode == 'human':\n",
        "#             if self.viewer is None:\n",
        "#                 self.viewer = rendering.SimpleImageViewer()\n",
        "# #             self.viewer.imshow(np.asarray(current_frame)*255)\n",
        "#             self.viewer.imshow(np.asarray(map_copy))\n",
        "#             return self.viewer.isopen\n",
        "    \n",
        "    def close(self):\n",
        "        pass\n",
        "        # if self.viewer is not None:\n",
        "        #     self.viewer.close()\n",
        "        #     print(self.viewer.isopen)\n",
        "        #     self.viewer = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsu91_DzuOVb",
        "colab_type": "text"
      },
      "source": [
        "# Model helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWQiadkRuOVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    sample_tuples = random.sample(self.storage, batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = tuple(zip(*sample_tuples))\n",
        "    return np.array(batch_states), batch_next_states, batch_actions, batch_rewards, batch_dones\n",
        "    \n",
        "def evaluate_policy(policy, env, eval_episodes=3):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    # print(\"Observatin shape from evaluate policy\")\n",
        "    # print(type(obs))\n",
        "    # print(obs.shape)\n",
        "    done = False\n",
        "    # print(\"Entering while loop\")\n",
        "    while not done:\n",
        "      action = policy.select_action(obs)\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wkltbrJuOWg",
        "colab_type": "text"
      },
      "source": [
        "# Model classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh14D50ouOWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Compute output size of convolution layer with given kernel size and stride\n",
        "    Assumes no padding\n",
        "\"\"\"\n",
        "def conv2d_size_out(size, kernel_size = 3, stride = 2, padding = 1):\n",
        "    return int((size - kernel_size + (2*padding))/stride) + 1\n",
        "\n",
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=2, padding = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(4)\n",
        "    self.conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=2, padding = 1)\n",
        "    self.bn2 = nn.BatchNorm2d(8)\n",
        "    #self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding = 1)\n",
        "    #self.bn3 = nn.BatchNorm2d(32)\n",
        "    \n",
        "    #conv3_output_size = conv2d_size_out(conv2d_size_out(conv2d_size_out(state_dim)))\n",
        "    conv3_output_size = conv2d_size_out(conv2d_size_out(state_dim))\n",
        "    linear_input_size = conv3_output_size * conv3_output_size * 8\n",
        "    # print(\"Linear input size : \", linear_input_size)\n",
        "    \n",
        "    self.head = nn.Linear(linear_input_size, action_dim)\n",
        "    self.max_action = max_action\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    # x = F.relu(self.bn3(self.conv3(x)))\n",
        "    # with torch.no_grad():\n",
        "    #     print(\"Shape x after convolutions: \" + str(x.shape))\n",
        "    #     print(\"Flattened x shape: \" + str(x.view(x.size(0), -1).shape))\n",
        "#         print(\"Shape of output after FC layer\")     \n",
        "    \n",
        "    return self.max_action * torch.tanh(self.head(x.view(x.size(0), -1)))\n",
        "    \n",
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Critic, self).__init__()\n",
        "    self.max_action = max_action\n",
        "    \n",
        "    # conv3_output_side_size = conv2d_size_out(conv2d_size_out(conv2d_size_out(state_dim)))\n",
        "    # conv3_total_output_size = conv3_output_side_size * conv3_output_side_size * 32\n",
        "    \n",
        "    conv2_output_side_size = conv2d_size_out(conv2d_size_out(state_dim))\n",
        "    conv2_total_output_size = conv2_output_side_size * conv2_output_side_size * 8\n",
        "    \n",
        "    \n",
        "    # Defining the first Critic neural network\n",
        "    self.critic1_conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=2, padding = 1)\n",
        "    self.critic1_bn1 = nn.BatchNorm2d(4)\n",
        "    self.critic1_conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=2, padding = 1)\n",
        "    self.critic1_bn2 = nn.BatchNorm2d(8)\n",
        "    # self.critic1_conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding = 1)\n",
        "    # self.critic1_bn3 = nn.BatchNorm2d(32) \n",
        "    self.critic1_head = nn.Linear(conv2_total_output_size+1, 1)\n",
        "    # Critic gives out only 1 value hence the output dimension is one\n",
        "    # Critic also takes action as input which is a scalar, hence adding 1 to linear_input_size\n",
        "    \n",
        "    # Defining the second Critic neural network\n",
        "    self.critic2_conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=2, padding = 1)\n",
        "    self.critic2_bn1 = nn.BatchNorm2d(4)\n",
        "    self.critic2_conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=2, padding = 1)\n",
        "    self.critic2_bn2 = nn.BatchNorm2d(8)\n",
        "    # self.critic2_conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding = 1)\n",
        "    # self.critic2_bn3 = nn.BatchNorm2d(32)\n",
        "    self.critic2_head = nn.Linear(conv2_total_output_size+1, 1)\n",
        "    # Critic gives out only 1 value hence the output dimension is one\n",
        "    # Critic also takes action as input which is a scalar, hence adding 1 to linear_input_size\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    ###############\n",
        "    ## Critic 1\n",
        "    ###############\n",
        "    # Pass through convolutional layers\n",
        "    x1 = F.relu(self.critic1_bn1(self.critic1_conv1(x)))\n",
        "    x1 = F.relu(self.critic1_bn2(self.critic1_conv2(x1)))\n",
        "    # x1 = F.relu(self.critic1_bn3(self.critic1_conv3(x1)))\n",
        "    x1 = x1.view(x1.size(0), -1)\n",
        " \n",
        "    #Concatenate action with the output of the convolutional layers\n",
        "    x1 = torch.cat([x1, u], 1)\n",
        "    \n",
        "    #Pass through FC layer\n",
        "    x1 = self.critic1_head(x1)\n",
        "\n",
        "    ###############\n",
        "    # Critic 2\n",
        "    ###############\n",
        "    \n",
        "    #Pass through convolutional layers\n",
        "    x2 = F.relu(self.critic2_bn1(self.critic2_conv1(x)))\n",
        "    x2 = F.relu(self.critic2_bn2(self.critic2_conv2(x2)))\n",
        "    # x2 = F.relu(self.critic2_bn3(self.critic2_conv3(x2)))\n",
        "    x2 = x2.view(x1.size(0), -1)\n",
        " \n",
        "    #Concatenate action with the output of the convolutional layers\n",
        "    x2 = torch.cat([x2, u], 1)\n",
        "    \n",
        "    #Pass through FC layer\n",
        "    x2 = self.critic2_head(x2)\n",
        "    \n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):   \n",
        "    ###############\n",
        "    ## Critic 1\n",
        "    ###############\n",
        "    # Pass through convolutional layers\n",
        "    x1 = F.relu(self.critic1_bn1(self.critic1_conv1(x)))\n",
        "    x1 = F.relu(self.critic1_bn2(self.critic1_conv2(x1)))\n",
        "    # x1 = F.relu(self.critic1_bn3(self.critic1_conv3(x1)))\n",
        "    x1 = x1.view(x1.size(0), -1)\n",
        " \n",
        "    #Concatenate action with the output of the convolutional layers\n",
        "    x1 = torch.cat([x1, u], 1)\n",
        "    \n",
        "    #Pass through FC layer\n",
        "    x1 = self.critic1_head(x1)\n",
        "    \n",
        "    return x1\n",
        "\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action, device):\n",
        "    self.device = device\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(self.device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim, max_action).to(self.device)\n",
        "    self.critic_target = Critic(state_dim, action_dim, max_action).to(self.device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state).to(self.device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()[0]\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    print( \"Replay buffer size at beginning of call to train: \" + str(len(replay_buffer.storage)) )\n",
        "    actor_conv1_frozen = self.actor.conv1.weight.data.clone().detach()\n",
        "    actor_conv2_frozen = self.actor.conv2.weight.data.clone().detach()\n",
        "    actor_head_frozen = self.actor.head.weight.data.clone().detach()\n",
        "\n",
        "    for it in range(iterations):\n",
        "      if(it%1000 == 0):\n",
        "        print(\"Training iteration local: \" + str(it))\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      \n",
        "      # Each element obtained from the replay_buffer is a np array\n",
        "      # Each element of the batch_states, batch_next_states is of the shape (1,1,size,size)\n",
        "      # Batch_states and batch_next_states have shapes (batch_size,1,1,size,size)\n",
        "      # What we want to feed to the conv networks is (batch_size, 1, size, size)\n",
        "      # Hence we squeeze out the first dimension ( zero'th dimension of size batch_size is intact )\n",
        "\n",
        "      # Each element of batch_rewards, batch_actions, batch_dones is a scalar\n",
        "      # We need to give a column vector to pytorch to compute losses\n",
        "      # Hence scalars go through a reshape(-1,1) ( eg: [1,2,3] to [[1],[2],[3]] )\n",
        "\n",
        "      state = torch.Tensor(batch_states).squeeze(1).to(self.device)\n",
        "      next_state = torch.Tensor(batch_next_states).squeeze(1).to(self.device) \n",
        "      action = torch.Tensor(batch_actions).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      reward = torch.Tensor(batch_rewards).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      done = torch.Tensor(batch_dones).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      # noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(self.device)\n",
        "      # noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      # noise = noise.reshape(-1,1)\n",
        "      noise_distribution = torch.distributions.normal.Normal(0, policy_noise)\n",
        "      noise = noise_distribution.sample(torch.Size([batch_size])).clamp(-noise_clip,noise_clip).view(-1,1).to(self.device)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # print(\"Action shape: \" + str(action.shape))\n",
        "      # print(\"Next action shape: \" + str(next_action.shape))\n",
        "        \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "#       print(\"Current Q1: \", current_Q1)\n",
        "#       print(\"Current Q2: \", current_Q2)\n",
        "#       print(\"Target Q: \", target_Q)\n",
        "\n",
        "#       exit(0)\n",
        "    \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:        \n",
        "\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        # print(\"Actor loss: \" + str(actor_loss))\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()        \n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "    \n",
        "    #Compute change in weights after the training iterations\n",
        "    actor_conv1_total_change = torch.sum( (torch.abs(self.actor.conv1.weight.data - actor_conv1_frozen) ) ).detach().cpu().numpy().flatten()[0]\n",
        "    actor_conv2_total_change = torch.sum( (torch.abs(self.actor.conv2.weight.data - actor_conv2_frozen) ) ).detach().cpu().numpy().flatten()[0]\n",
        "    actor_head_total_change = torch.sum( (torch.abs(self.actor.head.weight.data - actor_head_frozen) ) ).detach().cpu().numpy().flatten()[0]\n",
        "\n",
        "    print(\"Actor Conv 1 total change: \", actor_conv1_total_change )\n",
        "    print(\"Actor Conv 2 total change: \", actor_conv2_total_change )\n",
        "    print(\"Actor head total change: \", actor_head_total_change )\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP1Fz8VIuOXS",
        "colab_type": "text"
      },
      "source": [
        "# Generic helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyg0Ovf4uOXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoGE7cekuOYG",
        "colab_type": "text"
      },
      "source": [
        "# Env declaration and variable setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgmD5x8DuOYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "40fa916d-5773-4f66-ff34-85c647248483"
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Setting params\n",
        "env_name = \"CityMap\"\n",
        "seed = 0 # Random seed number\n",
        "#TODO: Change start_timestamp to 1e4 again\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e4 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0 #0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
        "train_iterations = 1000 # Number of iterations to run the training cycle for each time an episide is over\n",
        "\n",
        "\n",
        "#File name for actor and critic model saved files\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "#Create folder to save trained models\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")\n",
        "\n",
        "#Create the environment\n",
        "citymap = Image.open(\"MASK1.png\")\n",
        "roadmask = Image.open(\"MASK1.png\").convert('L')\n",
        "car_image = Image.open(\"car_upright.png\")\n",
        "car_image_width, car_image_height = car_image.getbbox()[2:4]\n",
        "car_image_resized = car_image.resize( (int(car_image_width/2), int(car_image_height/2)) )\n",
        "\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "\n",
        "env = CityMap(citymap, roadmask, car_image_resized)\n",
        "# env = wrappers.Monitor(env, monitor_dir, force = True, video_callable=lambda episode_id: True)\n",
        "# env._max_episode_steps = 200\n",
        "# print(env.reset())\n",
        "\n",
        "# Set seeds and get get info to initiate classes\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_window_size\n",
        "action_dim = 1\n",
        "max_action = env.max_turn_radians\n",
        "\n",
        "#Create the policy network\n",
        "policy = TD3(state_dim, action_dim, max_action, device)\n",
        "\n",
        "#Create the experience replay buffer\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "#Initialise variables\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "episode_timesteps = 1e3\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_CityMap_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF88u9Qfy7hE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "43b83642-552c-4cd7-8908-e154fde9112f"
      },
      "source": [
        "%%time\n",
        "#List for storing model evaluations\n",
        "evaluations = [evaluate_policy(policy, env)]\n",
        "# evaluations = [1]\n",
        "evaluation_timesteps = [0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -24542.264158\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mJNNVzkveop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40076fac-129b-4aa5-96a6-84516fb07503"
      },
      "source": [
        "device"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2zusv5ZuOZJ",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mi8H2dguOZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "cbd67cfa-06fd-4ec3-b361-8705943306a3"
      },
      "source": [
        "#Training\n",
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # print(\"Timesteps - total: \" + str(total_timesteps) + \"; done: \" + str(done) ) \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # Complete the recording\n",
        "    # env.stats_recorder.save_complete()\n",
        "    # env.stats_recorder.done = True\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, train_iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy, env))\n",
        "      evaluation_timesteps.append(total_timesteps)\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()[0]\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(obs)\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "  # We check if the episode is done\n",
        "  done_bool = 1 if episode_timesteps + 1 == env.max_episode_steps else float(done)\n",
        "  done = bool(done_bool)\n",
        "\n",
        "  # if(episode_timesteps == 199):\n",
        "  #   print(\"Reached 199 episode length\")\n",
        "  #   print(\"done: \"  +str(done))\n",
        "\n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "env.close()\n",
        "# Complete the recording\n",
        "# env.stats_recorder.save_complete()\n",
        "# env.stats_recorder.done = True\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy, env))\n",
        "evaluation_timesteps.append(total_timesteps)\n",
        "\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 2657 Episode Num: 1 Reward: -54067.992003320374\n",
            "Replay buffer size at beginning of call to train: 2657\n",
            "Training iteration local: 0\n",
            "Actor Conv 1 total change:  0.33088517\n",
            "Actor Conv 2 total change:  2.0929341\n",
            "Actor head total change:  27.046658\n",
            "Total Timesteps: 12657 Episode Num: 2 Reward: -288475.492304047\n",
            "Replay buffer size at beginning of call to train: 12657\n",
            "Training iteration local: 0\n",
            "Actor Conv 1 total change:  0.13997826\n",
            "Actor Conv 2 total change:  1.2231523\n",
            "Actor head total change:  1.3359303\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -14090.997877\n",
            "---------------------------------------\n",
            "Total Timesteps: 22657 Episode Num: 3 Reward: -17962.28579892931\n",
            "Replay buffer size at beginning of call to train: 22657\n",
            "Training iteration local: 0\n",
            "Actor Conv 1 total change:  0.0014882397\n",
            "Actor Conv 2 total change:  0.0150577165\n",
            "Actor head total change:  0.016847823\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -19911.442409\n",
            "---------------------------------------\n",
            "Total Timesteps: 32657 Episode Num: 4 Reward: -31542.275771715467\n",
            "Replay buffer size at beginning of call to train: 32657\n",
            "Training iteration local: 0\n",
            "Actor Conv 1 total change:  0.00034559984\n",
            "Actor Conv 2 total change:  0.004160242\n",
            "Actor head total change:  0.004757937\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -14418.906922\n",
            "---------------------------------------\n",
            "Total Timesteps: 42657 Episode Num: 5 Reward: -15660.749699087008\n",
            "Replay buffer size at beginning of call to train: 42657\n",
            "Training iteration local: 0\n",
            "Actor Conv 1 total change:  0.013904573\n",
            "Actor Conv 2 total change:  0.14607485\n",
            "Actor head total change:  0.15821254\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -12623.699056\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -5259.321573\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnQFT1FwuOZ2",
        "colab_type": "text"
      },
      "source": [
        "# Plot evaluation score over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owbXJ_PVuOZ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "9c90e046-7a00-43af-b66d-1e704f396d81"
      },
      "source": [
        "fig, ax = plt.subplots(1,1, figsize = (12,9))\n",
        "ax.plot( evaluation_timesteps, evaluations )\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_xlabel('Timesteps')\n",
        "ax.set_title('Agent score vs iterations');"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvjGbqqk2Hf-",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrzsQgYV2IY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "8071fd77-b5b6-498b-dfae-f924df890789"
      },
      "source": [
        "policy_inference = TD3(state_dim, action_dim, max_action, device)\n",
        "policy_inference.load(file_name, './pytorch_models/')\n",
        "\n",
        "env_inference = CityMap(citymap, roadmask, car_image_resized)\n",
        "env_inference = wrappers.Monitor(env_inference, monitor_dir, force = True, video_callable=lambda episode_id: True)\n",
        "\n",
        "avg_reward_inference = evaluate_policy(policy_inference, env_inference, eval_episodes=3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -12143.621939\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p19dva2I25G8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "974c17e0-b434-4502-a2bb-0b5f3e717e00"
      },
      "source": [
        "avg_reward_inference"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12143.621938766628"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FuqNdQF3ejE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}