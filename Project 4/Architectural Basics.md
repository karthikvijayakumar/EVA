1. How many layers
<br>This is goverened by 2 things. First is expressivity and the second is receptive field. One needs enough layers to be able to form edgers, gradients and patterns, this is expressivity. The layers that feeds into the final output layer should have seen the full image and not just a section of it.
2. 3x3 Convolutions
<br>3x3 convolutions are 3x3 matrices which get applied on top of the input image ( to a given layer ). Generally its a 3x3xn matrix where n is the number of channels in the input image. Each pixel in the resulting image is a linear combination of 3x3xn pixels in the input image ( to the layer ).

3. Kernels and how do we decide the number of kernels?
<br> The number of kernels is the number of convolution matrices we choose to use in our network ( assuming we restrict ourselves to the context from class ). This is governed by the expressivity of the network. The more complex the use case, the more edges, gradients and patterns we need to detect to meet our goal and the more kernels we would need.

4. Receptive Field
<br> At any given layer ( before flattening ) we have an image of some size with us. The receptive field could be defined as follows. Take a pixel in the image in the current layer, this pixel is derived from a sub image of the input image to the network. The size of that sub-image is the receptive field.

5. SoftMax
<br> Given an array of values [a_1, a_2, ..., a_n], the softmax translates this array into [ exp(a_i)/(exp(a_1)+...exp(a_n)); i = 1...n  ]. Its key to note that while the values generated by softmax are between 0-1 they are probability like and not actual probabilities.

<br> Sections 1-5 are ordered such since they are absolute minimum required to define a convolutional neural network. Next we see a few practical aspects of training before adding other operations/features to our models

6. Number of Epochs and when to increase them
<br>An epoch is a run over all the training images. With multiple epochs we run through the training datasets as many times.
<br>This is governed by 2 things:
    1. Is the loss function actually decreasing after each epoch
    <br>If the loss function isn't significantly decreasing with epochs then there are likely issues with the network architecture and letting gradient descent run for longer isnt going to very productive.
    2. Computational time/energy/power
    <br>Of course as one can imagine, the more epochs one lets the training run the more training time and energy it takes. For example with a poor architecture one could take a few weeks to train a model for the MNIST dataset. Clearly that isnt very prudent

7. How do we know our network is not going well, comparatively, very early
<br> If the training accuracy is not close to our target or the state-of-the-art ( with such architectures ) after the first couple of epochs, then likely we need to change something with the network instead of letting the training run for more epochs. The actual accuracies/loss function values are ofcourse subjective. For some problems even the state-of-the-art could be say 60% accuracy and for some it could 99.99999% accuracy

8. When to add validation checks
<br>Always!! Always measure the validation accuracy after each epoch during the training process to identify if the model is overfitting early on.

<br>Now we look at operations beyond 3x3 convolutions

9. MaxPooling
Maxpooling with a nxn kernel is similar to a convolution operation. Except that instead of a linear sum we choose the max of those nxn pixels.
<br>Given a nxnxm image ( m channels ), max pooling acts on each channel independently

10. 1x1 Convolutions
<br>1x1 convolutions act on each individual pixel and perform a linear combination on the channels. This lets us decrease/increase the number of channels without affecting image size or receptive field etc.

11. Concept of Transition Layers
Transition layers are layers were we consolidate features built up so far ( edges->gradients->patterns ). In a transition layer we select the best/most pertinent subset of the features generated so far and pass it on the next set of convolutions. 

12. Position of Transition Layer
<br>Transition layers come after atleast 2 convolutions typically and atleast 2 convolution layers before the output layer.

13. Position of MaxPooling
<br>Maxpooling operations sit in the transition layer and follow the same placement as transition layers.

14. Batch Normalization
<br>What we perform when training neural networks is typically stochastic mini-batch gradient descent. In this one takes a random subset of the training images, pases them through the network and backpropogates it to find gradients for the parameters and adjust them accordingly.
<br>Our networks contain activation functions like relu or sigmoid. Across random subsets of the training images the outputs of a given layer post the activation function can change considerably and this could confuse the next layer. For example in one subset of images one portion of the image could be black and in the other it could be white. Batch normalization tries to aleviate this problem by normalizing the images obtained after each batch by comparing all the images in a given batch.

15. Image Normalization
<br>Given an image normalize the pixel values to be between 0-1 ( or 0-225 etc ) with some variance. Batch normalization is image normalization applied after each layer in each batch of training.
16. The distance of MaxPooling from Prediction
<br> Maxpooling should be atleast 2 layers from the output layer. This is to ensure we dont lose out on any feature built up so far before the predction layer.

17. The distance of Batch Normalization from Prediction
<br>Batch normalization shouldnt be used in the penultimate convolutional layer before flattening.

18. DropOut
<br> Dropout is a process where in the course of training a random subset of pixels are turned off when feeding into the next layer ( at each layer ). One can control the size of the random subset that will be turned off through a parameter.

19. When do we introduce DropOut, or when do we know we have some overfitting
<br>We can observe overfitting when there is considerable difference between training and validation accuracy/loss function values.

<br> Now that we have introduced batch normalization, drop out and transition layers, lets shift our attention back to the training process

29. Learning Rate
<br>Any gradient descent involves the use of a learning rate/step size. The size by which the parameters get updated is proportional to learning rate.

21. LR schedule and concept behind it
<br> As one progresses through the training process the loss function values get progressively smaller and the size of the update required to the parameters also change accordingly. Hence using a fixed learning rate is not the most effective. LR schedulers provide a way to control the learning rate as a function of the epoch number.

22. When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)
<br> When we notice that the training error/accuracy is not improving sufficiently with epochs. This could indicate the network doesnt have the expressivity required for the problem at hand. This could mean on needs more kernels at each layer or one just needs more layers in the first place.

23. Batch Size, and effects of batch size
<br> Batch size controls the number of images in the batch of stochastic mini-batch gradient descent. Larger the batch size, lesser the number of batches per epoch.
<br>Larger batch sizes lead to faster execution since lesser updates are made to the parameters. However that also means the updates are coarser and hence the training parameters might not reach the optimal value they should.

24. Adam vs SGD
<br>Adam and SGD are two different optimizers. Optimizers control how the parameter values need to be updated given a gradient.
<br>In my limited understanding SGD is a more vanilla parameter update mechanism ( it could momentum and get more complex ) compared to Adam. SGD doesnt differentiate between different paramters in the model. Adam on the other hand observes which parameters are getting updated more frequently and appropriately boosts or suppresses their change.